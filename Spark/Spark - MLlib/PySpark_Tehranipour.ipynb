{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark tutorial"
      ],
      "metadata": {
        "id": "FtpWRmsyCVh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is PySpark?\n"
      ],
      "metadata": {
        "id": "GqikQHnxCVe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pyspark is a python api for working with apache spark.\n",
        "\n",
        "* **Python api**: you can use the syntex and agility of python to interact with and send commands to a system that is not based, at its core, on python.\n",
        "\n",
        "* **Apache Spark**: a system designed for working, analyzing and modeling with immense amounts of data in many computers at the same time. Putting it in a different way, apache spark allows you to run computations in parallel, instead of sequentially. It allows you to divide one incredibly large task into many smaller tasks, and run each such task on a different machine. This allowes you to accomplish your analysis goals in reasonable time that would not be possible on a single machine.\n",
        "\n",
        "usually, we would define the amount of data that suits PySpark as what would not fit into a single machine storage (let alone RAM).\n",
        "\n",
        "**important related concepts:**\n",
        "1. distributed computing - when you distribute a task into several smaller task that run at the same time. this is what pyspark allows you to do with many machines, but it can also be done on a single machine with several threads, for example.\n",
        "2. cluster - a network of machines that can take on tasks from a user, interact with one another and return results. these provide the computing resources that pyspark will use to make the computations.\n",
        "3. Resilient Distributed Dataset (RDD) - an immutable distributed collection of data. it is not tabular and has no data schema. therefore, for tabular data wrangling, DataFrames allowes for more API options and uner-then-hood optimizations. still, you might encounter RDDs as you learn more about Spark, and should be aware of their existence.\n",
        "\n",
        "**Part of PySpark we will cover:**\n",
        "1. PySpark SQL - contains commands for data processing and manipulation.\n",
        "2. PySpark MLlib - includes a variety of models, model training and related commands.\n",
        "\n",
        "**Spark Architecture:**\n",
        "to send commands and receive results from a cluster, you will need to initiate a spark session. this object is your tool for interacting with Spark. each user of the cluster will have its own Spark Session, that will allow him to use the cluster in isolation from other users. all of the sessions are communicating with a spark context, which is the master node in the cluster - that is, it assigns each of computers in the cluster tasks and coordinates them. each of the computers in the cluster that perform tasks for a master node is called a worker node. to connect to a worker node, the master node needs to get that node's comput power allocated to it, by a cluster manager, that is responsable for distributing the cluster resources. inside each worker node, there are execute programs that run the tasks - they can run multiple tasks simultaneously, and has their own cashe for storing results. so, each master node can have multiple worker nodes, that can have multiple tasks running.  "
      ],
      "metadata": {
        "id": "W5Lv9IbvCRm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -a"
      ],
      "metadata": {
        "id": "ZAaYh3aYFa42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark # ~ 1 min"
      ],
      "metadata": {
        "id": "c4FEW2kOCNS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-GObLMvhH1h"
      },
      "outputs": [],
      "source": [
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q https://www-us.apache.org/dist/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "# !tar xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "# !pip install -q findspark\n",
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiW1NHVriHmG"
      },
      "source": [
        "# Spark DataFrame Basics\n",
        "\n",
        "Spark DataFrames allow for easy handling of large datasets.\n",
        "\n",
        "* Easy syntax\n",
        "* Ability to use SQL directly in the dataframe\n",
        "* Operations are automatically distributed across RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hqXqLiqjBOi"
      },
      "source": [
        "## Create a DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PofF_8DiZMe"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yMumuTnlvqV"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"pyspark_basics\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DvJFAnJmgwo"
      },
      "outputs": [],
      "source": [
        "%%writefile user_simple.json\n",
        "{\"name\":\"Bob\"}\n",
        "{\"name\":\"Jim\", \"age\":40}\n",
        "{\"name\":\"Mary\", \"age\": 24}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_eO4v0NmEju"
      },
      "outputs": [],
      "source": [
        "df = spark.read.json(\"user_simple.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Ocoonbj2GEWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "VUVPNUquGGev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAc8qcdHkkQ4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfKSE6KJjFuq"
      },
      "source": [
        "## Show DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQMevRT-jK4J"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF2i36PsnLlC"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7mJ1ZFxnPiF"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GummdOXQlK9Q"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgTEqsABnhlZ"
      },
      "outputs": [],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gYzymrllHbTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_unxKKfjmy7"
      },
      "source": [
        "## Specifying Schema Structure\n",
        "\n",
        "- Some data types make it easier to infer schema.\n",
        "\n",
        "- Often have to set the schema yourself\n",
        "\n",
        "- Spark has tools to help specify the structure\n",
        "\n",
        "Next we need to create the list of Structure fields\n",
        "  * :param name: string, name of the field.\n",
        "  * :param dataType: :class:`DataType` of the field.\n",
        "  * :param nullable: boolean, whether the field can be null (None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwpWKuOsn0KC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTCd9Kn6oBAt"
      },
      "outputs": [],
      "source": [
        "data_schema = [StructField(\"age\", IntegerType(), True), StructField(\"name\",StringType(), True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B-U5Uy9oOJ1"
      },
      "outputs": [],
      "source": [
        "final_struc = StructType(fields=data_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRceAk5OoZ4a"
      },
      "outputs": [],
      "source": [
        "df = spark.read.json(\"user_simple.json\", schema=final_struc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1JV1wX-ohr_"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bxUbj1NmwCn"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BstAkwekUW4"
      },
      "source": [
        "## Grab Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BXtJ8Vtjqor"
      },
      "outputs": [],
      "source": [
        "df['age']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAVvHNehowFL"
      },
      "outputs": [],
      "source": [
        "type(df['age'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEzDx_AKox5b"
      },
      "outputs": [],
      "source": [
        "df.select(\"age\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8Kq6dcmpADD"
      },
      "outputs": [],
      "source": [
        "type(df.select(\"age\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKdekPhcpCms"
      },
      "outputs": [],
      "source": [
        "df.select(\"age\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXNWVdjZpK-K"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhtn6i2hpd2v"
      },
      "outputs": [],
      "source": [
        "df.select([\"name\",\"age\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPCk9uufphpK"
      },
      "outputs": [],
      "source": [
        "df.select([\"name\",\"age\"]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOXJGzI1kYA8"
      },
      "source": [
        "## Create New Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT6NHjD9kZ-u"
      },
      "outputs": [],
      "source": [
        "df.withColumn(\"newAge\", df['age']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IGHHPbQqEww"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qKwhBTjqNuc"
      },
      "outputs": [],
      "source": [
        "df.withColumnRenamed(\"name\",\"firstName\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uco9P30jqXii"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKCCXXy-qbqC"
      },
      "outputs": [],
      "source": [
        "df.withColumn(\"agePlusTen\", df['age']+10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYrscKO8qyJd"
      },
      "outputs": [],
      "source": [
        "df.withColumn(\"age_minus_5\", df['age']-5).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRgWZeRekasQ"
      },
      "source": [
        "## Using SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "489jaijnkfno"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"custmers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEPRi6zorG8Q"
      },
      "outputs": [],
      "source": [
        "sql_results = spark.sql(\"SELECT * from custmers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f2j94UKrNpt"
      },
      "outputs": [],
      "source": [
        "sql_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wcBefx-rO9K"
      },
      "outputs": [],
      "source": [
        "sql_results.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuVx3fkfrRJy"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * FROM custmers WHERE age=24\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHN9Ff1qr1GB"
      },
      "source": [
        "## DataFrame Operations\n",
        "\n",
        "- Cover basic operations with Spark DataFrames.\n",
        "- Use stock data from Walmart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XA4cW1Gr3ni"
      },
      "outputs": [],
      "source": [
        "!curl https://raw.githubusercontent.com/markumreed/colab_pyspark/main/WMT.csv >> WMT.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uysJywW3dgUj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"operations\").getOrCreate()\n",
        "df = spark.read.csv('WMT.csv',inferSchema=True,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEd4eEX4fp1I"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52TTDejRpU_D"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A6KhjDOfKwz"
      },
      "source": [
        "## Filtering Data\n",
        "\n",
        "- DataFrames allow for quick filtering of data based on conditions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaP5wGURfnfg"
      },
      "outputs": [],
      "source": [
        "df.filter('Close<62').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef9akX3opx-z"
      },
      "outputs": [],
      "source": [
        "df.filter('Close<62').select('Open').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYUFceU6r0nI"
      },
      "outputs": [],
      "source": [
        "df.filter('Close<62').select(['Date','Open']).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9ISe-A8fxUL"
      },
      "source": [
        "## Using Comparison Operators\n",
        "- Using comparison operators will look similar to SQL operators\n",
        "- Make to call the entire column within the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCwmv8AegBVz"
      },
      "outputs": [],
      "source": [
        "df.filter(df['Close'] < 62).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF4kkYnRsL3q"
      },
      "outputs": [],
      "source": [
        "df.filter((df['Close'] < 62) & ~(df['Open'] > 60)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5_fKXo5sac2"
      },
      "outputs": [],
      "source": [
        "df.filter(df['Open'] == 60.98).show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJhfhLIatH3C"
      },
      "outputs": [],
      "source": [
        "df.filter(df['Open'] == 60.98).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGsDRRCutgUx"
      },
      "outputs": [],
      "source": [
        "res =df.filter(df['Open'] == 60.98).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfmZMsaItlXh"
      },
      "outputs": [],
      "source": [
        "type(res[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxFzHkFTtnXo"
      },
      "outputs": [],
      "source": [
        "res[0].asDict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDHaBsTIts-7"
      },
      "outputs": [],
      "source": [
        "for item in res[0]:\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUb_M36pt459"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdEFN1pst8Zk"
      },
      "outputs": [],
      "source": [
        "pd.Series(res[0].asDict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH04CrhILKaH"
      },
      "source": [
        "# GroupBy and Aggregate Functions\n",
        "- `GroupBy` allows you to group rows together based off some column value\n",
        "- Once you've performed the `GroupBy` operation you can use an aggregate function off that data.\n",
        "- An aggregate function aggregates multiple rows of data into a single output, such as taking the sum of inputs, or counting the number of inputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dj8S7F5LeHO"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgyolVy-Sl__"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"groupbyagg\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVPgV-xyL9lW"
      },
      "source": [
        "## Import Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56wrqUq8MAOh"
      },
      "outputs": [],
      "source": [
        "!curl https://raw.githubusercontent.com/markumreed/colab_pyspark/main/sales_data.csv >> sales_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEgYTipcOh0r"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"sales_data.csv\", inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVjWzzsRS1-6"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl_XoAzbS4Hi"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMastDr_MBk0"
      },
      "source": [
        "## Grouping Data\n",
        "- Group the data by company"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF8gnLHcMExx"
      },
      "outputs": [],
      "source": [
        "df.groupBy(\"company\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NoOQ2BFMMMT"
      },
      "source": [
        "## Aggregate Functions\n",
        "- mean, count, max, min, sum..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otQZOtpSMP-Y"
      },
      "outputs": [],
      "source": [
        "df.groupBy(\"company\").mean().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMza7D7UTxJC"
      },
      "outputs": [],
      "source": [
        "df.groupBy(\"company\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DexQqbPT-QJ"
      },
      "outputs": [],
      "source": [
        "df.groupBy(\"company\").min().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrL9eCJxUGB4"
      },
      "outputs": [],
      "source": [
        "df.groupBy(\"company\").max().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpSvlEHiUR2j"
      },
      "outputs": [],
      "source": [
        "df.groupBy(\"company\").sum().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpI9QvtJMQwW"
      },
      "source": [
        "## Aggregating\n",
        "\n",
        "- Not all methods need a groupby call, instead you can just call the generalized `.agg()` method, that will call the aggregate across all rows in the dataframe column specified.\n",
        "- It can take in arguments as a single column, or create multiple aggregate calls all at once using dictionary notation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k-ZvnPvMgYr"
      },
      "outputs": [],
      "source": [
        "df.agg({\"num_sales\":\"max\"}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLVEMczZU4TB"
      },
      "outputs": [],
      "source": [
        "df.groupBy(\"company\").agg({\"num_sales\":\"mean\"}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmoEejcgVG53"
      },
      "outputs": [],
      "source": [
        "company_groups = df.groupBy(\"company\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BTEXsvJVMqM"
      },
      "outputs": [],
      "source": [
        "company_groups.min().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66d1zN6iMj3V"
      },
      "source": [
        "## Functions\n",
        "There are a variety of functions you can import from pyspark.sql.functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf1hR0XMMnzS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import countDistinct, avg, stddev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx7Mt_rWMqiK"
      },
      "outputs": [],
      "source": [
        "df.select(countDistinct(\"num_sales\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTtgXIJMWWHq"
      },
      "outputs": [],
      "source": [
        "df.select(avg(\"num_sales\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kETF6uvgWd88"
      },
      "outputs": [],
      "source": [
        "df.select(stddev(\"num_sales\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo2IWVdMMq0l"
      },
      "source": [
        "### Alias\n",
        "- To change the name, use the `.alias()` method for this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82U7lERVMytz"
      },
      "outputs": [],
      "source": [
        "df.select(countDistinct(\"num_sales\").alias(\"ANYTHING WE WANT\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJTrtVbiM1zj"
      },
      "source": [
        "### Precision\n",
        "- Use the `format_number` to change precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqgFZW_0NA7N"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import format_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSpARgeGWpCb"
      },
      "outputs": [],
      "source": [
        "sales_std = df.select(stddev(\"num_sales\").alias(\"stddev\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMg4hJ-KW2IC"
      },
      "outputs": [],
      "source": [
        "sales_std.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsk0g7wMW8NZ"
      },
      "outputs": [],
      "source": [
        "sales_std.select(format_number(\"stddev\",2)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdg0Rk-pNB51"
      },
      "source": [
        "## Order By\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI7g8LIfNFyE"
      },
      "outputs": [],
      "source": [
        "df.orderBy(\"num_sales\").show() # Ascending Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5utKqoxXPEl"
      },
      "outputs": [],
      "source": [
        "df.orderBy(df['num_sales'].desc()).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFwrfNMXhGGQ"
      },
      "source": [
        "# Missing Data\n",
        "\n",
        "- Often data sources are incomplete\n",
        "- There are 3 options for filling in missing data:\n",
        "  1. Just keep the missing data points.\n",
        "  1. Drop them missing data points/row\n",
        "  1. Fill them in with some other value.\n",
        "\n",
        "## Keeping the missing data\n",
        "A few machine learning algorithms can easily deal with missing data, let's see what it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWe9t-XXiFTq"
      },
      "outputs": [],
      "source": [
        "!curl https://raw.githubusercontent.com/markumreed/colab_pyspark/main/missing_data.csv >> missing_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DCCktgsjOPC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"missing_data\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY5afHKulmu4"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"missing_data.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq_9UowHlsDN"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9bKgVZ0l6Dz"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V94I3GA0iIr9"
      },
      "source": [
        "## Drop the missing data\n",
        "\n",
        "You can use the `.na` functions for missing data. The `drop` command has the following parameters:\n",
        "\n",
        "```df.na.drop(how='any', thresh=None, subset=None)```\n",
        "    \n",
        "    * param how: 'any' or 'all'.\n",
        "    \n",
        "        If 'any', drop a row if it contains any nulls.\n",
        "        If 'all', drop a row only if all its values are null.\n",
        "    \n",
        "    * param thresh: int, default None\n",
        "    \n",
        "        If specified, drop rows that have less than `thresh` non-null values.\n",
        "        This overwrites the `how` parameter.\n",
        "        \n",
        "    * param subset:\n",
        "        optional list of column names to consider.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbOl5osTjg0W"
      },
      "outputs": [],
      "source": [
        "df.na.drop().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLL9RqF_mlwF"
      },
      "outputs": [],
      "source": [
        "df.na.drop(thresh=2).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMrnoSccm0YP"
      },
      "outputs": [],
      "source": [
        "df.na.drop(subset=['sales']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvUXc4R0m_x6"
      },
      "outputs": [],
      "source": [
        "df.na.drop(how='any').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzfmG7DmnJTt"
      },
      "outputs": [],
      "source": [
        "df.na.drop(how='all').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kuY9DNajhIZ"
      },
      "source": [
        "## Fill the missing values\n",
        "\n",
        "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qkca01oTjjsk"
      },
      "outputs": [],
      "source": [
        "df.na.fill('سهیل').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QTnHF7xneE4"
      },
      "outputs": [],
      "source": [
        "df.na.fill(999).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj56GgkAnk14"
      },
      "outputs": [],
      "source": [
        "df.na.fill(\"Missing Name\", subset=[\"name\"]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsmlb7Yvn59E"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FjhZLrloAsz"
      },
      "outputs": [],
      "source": [
        "mean_value = df.select(mean(df['sales'])).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqFzudpCoG1q"
      },
      "outputs": [],
      "source": [
        "mean_sales_value = mean_value[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YLJpDMhoIv1"
      },
      "outputs": [],
      "source": [
        "df.na.fill(mean_sales_value, [\"sales\"]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lZ8Y0ziodT_"
      },
      "outputs": [],
      "source": [
        "# DON'T DO THIS\n",
        "df.na.fill(df.select(mean(df['sales'])).collect()[0][0] ,['sales']).show() # NOT EASY TO READ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5uWGuqMjkBq"
      },
      "source": [
        "# Dates and Timestamps\n",
        "\n",
        "You will often find yourself working with Time and Date information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hobk2LFjpDG"
      },
      "outputs": [],
      "source": [
        "!curl https://raw.githubusercontent.com/markumreed/colab_pyspark/main/WMT.csv >> WMT.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lrdxm8U4jtMB"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName('walmart_dates').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj3S5esdo3e2"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('WMT.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNCOshFHo9Ks"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_8FAcLko93g"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import format_number, dayofmonth, hour, dayofyear, month, year, weekofyear, date_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8ol3wSepN12"
      },
      "outputs": [],
      "source": [
        "df.select(dayofmonth(df['Date'])).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfZvFRfopUUI"
      },
      "outputs": [],
      "source": [
        "df.select(hour(df['Date'])).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZW7mEdVpjPH"
      },
      "outputs": [],
      "source": [
        "df.select(dayofyear(df['Date'])).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPb4Vf0Ipt3M"
      },
      "outputs": [],
      "source": [
        "df.select(month(df['Date'])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yI_Xo8ZqEya"
      },
      "source": [
        "Find Avg Close Price per month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br9nY4VVqABE"
      },
      "outputs": [],
      "source": [
        "df.withColumn(\"Month\", month(df['Date'])).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_DA8yroqRZD"
      },
      "outputs": [],
      "source": [
        "df2 = df.withColumn(\"Month\", month(df['Date']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVXTu-UCqWu6"
      },
      "outputs": [],
      "source": [
        "df2.groupBy(\"Month\").mean()[['avg(Month)', 'avg(Close)']].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEtqZO-uqkQ-"
      },
      "outputs": [],
      "source": [
        "res = df2.groupBy(\"Month\").mean()[['avg(Month)', 'avg(Close)']]\n",
        "res = res.withColumnRenamed(\"avg(Month)\", \"Month\")\n",
        "res = res.select(\"Month\", format_number('avg(Close)',2).alias(\"Mean Close\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ULQkBngS6gQ"
      },
      "source": [
        "# Spark DataFrames Review"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/markumreed/colab_pyspark/main/appl_stock.csv >> apple_stock.csv"
      ],
      "metadata": {
        "id": "K5Fa1XPM1Cz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN0fU_G_q810"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"apple_stock\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVgziEhLVRAP"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"apple_stock.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3A3xYKoVl2M"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfYreoDWVpmq"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd7ISAq3Vv6G"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuCwBTbIV7nm"
      },
      "outputs": [],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gInl6jFwWDQ4"
      },
      "outputs": [],
      "source": [
        "df.describe().printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L558GrEWNKY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import format_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qtslK8fWVkl"
      },
      "outputs": [],
      "source": [
        "res = df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGNZVAseWjz7"
      },
      "outputs": [],
      "source": [
        "df.describe().columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivqX5xYAWaSm"
      },
      "outputs": [],
      "source": [
        "res.select(res[\"summary\"],\n",
        "             format_number(res['Open'].cast('float'), 2).alias('Open'),\n",
        "             format_number(res['High'].cast('float'), 2).alias('High'),\n",
        "             format_number(res['Low'].cast('float'), 2).alias('Low'),\n",
        "             format_number(res['Close'].cast('float'), 2).alias('Close'),\n",
        "             res['Volume'] .cast('int').alias('Volume')\n",
        "             ).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrvxJateXGK-"
      },
      "outputs": [],
      "source": [
        "# High vs Volume\n",
        "df2 = df.withColumn(\"HV Ratio\", df['High']/df['Volume'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCuoYCD4XZus"
      },
      "outputs": [],
      "source": [
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcOCPNaZXajw"
      },
      "outputs": [],
      "source": [
        "df2.select('HV Ratio').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7ordPbjXhfx"
      },
      "outputs": [],
      "source": [
        "df.orderBy(df['High'].desc()).head(1)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ljoBGJJXssF"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "df.select(mean('Close')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsAdLi1CX2ZQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import max, min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzRB1VF1YAet"
      },
      "outputs": [],
      "source": [
        "df.select(max('Volume'), min('Volume')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TGenn_5YEwO"
      },
      "outputs": [],
      "source": [
        "df.filter(\"Close < 120\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH34sAmsYRYc"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UGRdhhkYa23"
      },
      "outputs": [],
      "source": [
        "res = df.filter('Close < 1210')\n",
        "res.select(count('Close')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s93rgt_vYj0P"
      },
      "outputs": [],
      "source": [
        "(df.filter('High > 180').count() * 1.0/df.count()) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYOFUjt_Y0tz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iWVCxWyY-kp"
      },
      "outputs": [],
      "source": [
        "df.select(corr('High', 'Volume')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9tZFmHSZCUw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import year\n",
        "yeardf = df.withColumn(\"Year\", year(df['Date']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOz73y1YZMQX"
      },
      "outputs": [],
      "source": [
        "max_df = yeardf.groupBy('Year').max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk1itiSsZPmo"
      },
      "outputs": [],
      "source": [
        "max_df.select('Year', 'max(High)').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpoJjT01ZXJV"
      },
      "outputs": [],
      "source": [
        "max_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v00PH2pPZUCV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIkbBH2AZjW1"
      },
      "outputs": [],
      "source": [
        "monthdf = df.withColumn(\"Month\", month(\"Date\"))\n",
        "monthavgs = monthdf.select(\"Month\", \"Close\").groupBy(\"Month\").mean()\n",
        "monthavgs.select(\"Month\", \"avg(Close)\").orderBy('Month').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrAk7sPLk-j0"
      },
      "source": [
        "# Linear Regression with PySpark\n",
        "\n",
        "- Based on the Official Spark Documentation for PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL8ktnqOlLdq"
      },
      "outputs": [],
      "source": [
        "!curl https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt >> sample_linear_regression_data.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miWKcrd3lOWY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvtcbtg5p-p4"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"lr_ex\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1ELzedBqDvq"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKoAoMQ-qNaP"
      },
      "outputs": [],
      "source": [
        "training = spark.read.format(\"libsvm\").load(\"sample_linear_regression_data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdcehtq3qZRb"
      },
      "outputs": [],
      "source": [
        "training.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA7YUGcAsfcv"
      },
      "outputs": [],
      "source": [
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-zKzjihtcFJ"
      },
      "outputs": [],
      "source": [
        "lrModel = lr.fit(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sICq8WhitijH"
      },
      "outputs": [],
      "source": [
        "print(\"Coefficients:\", str(lrModel.coefficients))\n",
        "print(\"Intercept:\", str(lrModel.intercept))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IP5jAbNtoen"
      },
      "outputs": [],
      "source": [
        "trainSummary = lrModel.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0W4dfOkt9jU"
      },
      "outputs": [],
      "source": [
        "print(\"MAE: \", trainSummary.meanAbsoluteError)\n",
        "print(\"MSE: \", trainSummary.meanSquaredError)\n",
        "print(\"RMSE: \", trainSummary.rootMeanSquaredError)\n",
        "print(\"R2: \", trainSummary.r2)\n",
        "print(\"Adj R2: \", trainSummary.r2adj)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp1AL6xSutsF"
      },
      "source": [
        "## Train Test Split with PySpark\n",
        "- Pass in the split between training/test as a list.\n",
        "-  No correct, but generally 70/30 or 60/40 splits are used.\n",
        "-  Depending on how much data you have and how unbalanced it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y9VxsAwt-2e"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"libsvm\").load(\"sample_linear_regression_data.txt\") # FULL DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n05TTVsHu126"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aviUru8PvOr9"
      },
      "outputs": [],
      "source": [
        "test_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebuDxrJdvRgo"
      },
      "outputs": [],
      "source": [
        "unlabeled_data = test_data.select('features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9_g2ezgvhBP"
      },
      "outputs": [],
      "source": [
        "corrected_model = lr.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su2Dxthbv4H2"
      },
      "outputs": [],
      "source": [
        "res = corrected_model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6G06ARSv-YR"
      },
      "outputs": [],
      "source": [
        "print(\"MAE: \", res.meanAbsoluteError)\n",
        "print(\"MSE: \", res.meanSquaredError)\n",
        "print(\"RMSE: \", res.rootMeanSquaredError)\n",
        "print(\"R2: \", res.r2)\n",
        "print(\"Adj R2: \", res.r2adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n9IDCFzwHZs"
      },
      "outputs": [],
      "source": [
        "predictions = corrected_model.transform(unlabeled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EISzcJmXwV_-"
      },
      "outputs": [],
      "source": [
        "predictions.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6MakK4CSEzE"
      },
      "source": [
        "# Data Transformations with PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up11ZCXeVpLD"
      },
      "source": [
        "## Data Features\n",
        "### StringIndexer\n",
        "- Convert string data into numerical (categorical feature)\n",
        "- Encode as dummy variables/OneHotEncoder\n",
        "- `StringIndexer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTx86qtuVyI2"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "df2 = spark.createDataFrame(\n",
        "    [(0,\"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"b\"), (5, \"c\")],\n",
        "    [\"user_id\", \"category\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R698LIYmZ5ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea1ba32-8d85-4dfd-9a52-c49e15bc0914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+\n",
            "|user_id|category|\n",
            "+-------+--------+\n",
            "|      0|       a|\n",
            "|      1|       b|\n",
            "|      2|       c|\n",
            "|      3|       a|\n",
            "|      4|       b|\n",
            "|      5|       c|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB6edwjhaIWP"
      },
      "outputs": [],
      "source": [
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0WiBd4KaYNW"
      },
      "outputs": [],
      "source": [
        "indexed = indexer.fit(df2).transform(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSvZEevjao_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0980079c-b800-4ded-ba3e-1f201df1d90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+-------------+\n",
            "|user_id|category|categoryIndex|\n",
            "+-------+--------+-------------+\n",
            "|      0|       a|          0.0|\n",
            "|      1|       b|          1.0|\n",
            "|      2|       c|          2.0|\n",
            "|      3|       a|          0.0|\n",
            "|      4|       b|          1.0|\n",
            "|      5|       c|          2.0|\n",
            "+-------+--------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexed.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQWkMmz9V6qn"
      },
      "source": [
        "## VectorIndexer\n",
        "- **VectorAssembler** is a transformer that combines a given list of columns into a single vector column.\n",
        "- **VectorAssembler** accepts the following input column types:\n",
        "  - all numeric types, boolean type, and vector type.  \n",
        "\n",
        "---\n",
        "\n",
        "- Assume that we have a DataFrame with the columns id, hour, mobile, userFeatures, and clicked:\n",
        "\n",
        "id | hour | mobile | userFeatures     | clicked\n",
        "----|------|--------|------------------|---------\n",
        "0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0\n",
        "     \n",
        "- userFeatures is a vector column that contains three user features.  \n",
        "- After transformation we should get the following DataFrame:\n",
        "\n",
        "id | hour | mobile | userFeatures     | clicked | features\n",
        "----|------|--------|------------------|---------|-----------------------------\n",
        "0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0     | [18.0, 1.0, 0.0, 10.0, 0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1aTeRAGWipt"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyBSoLYKbwBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7af3ad-4053-40ac-f99e-cd21b0746e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+--------------+-------+\n",
            "| id|hour|mobile|  userFeatures|clicked|\n",
            "+---+----+------+--------------+-------+\n",
            "|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|\n",
            "+---+----+------+--------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df3 = spark.createDataFrame(\n",
        "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
        "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"]\n",
        ")\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_cT_4yvcE5d"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols = [\"hour\", \"mobile\", \"userFeatures\"],\n",
        "    outputCol = \"features\"\n",
        ")\n",
        "output = assembler.transform(df3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMKqn4MLce0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01595834-dcbb-42e6-9481-d4d108589384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+\n",
            "|            features|clicked|\n",
            "+--------------------+-------+\n",
            "|[18.0,1.0,0.0,10....|    1.0|\n",
            "+--------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "output.select(\"features\", \"clicked\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark - Linear Regression (Another example)"
      ],
      "metadata": {
        "id": "_OgLelIrPjPS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilX6-se1-2RQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45Hg0LCV-4fq"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"lin_reg\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwGFIQnS-9Lq"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"/content/drive/MyDrive/PySpark/ecommerce.csv\", inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-b3fIs4_Gxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82428241-6a3c-49cd-a6a3-1cb95968d341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Email: string (nullable = true)\n",
            " |-- Address: string (nullable = true)\n",
            " |-- Avatar: string (nullable = true)\n",
            " |-- Avg Session Length: double (nullable = true)\n",
            " |-- Time on App: double (nullable = true)\n",
            " |-- Time on Website: double (nullable = true)\n",
            " |-- Length of Membership: double (nullable = true)\n",
            " |-- Yearly Amount Spent: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB_QFnbv_J_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc83d37-2adf-49df-fb6e-be96c52dcfc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+----------------+------------------+------------------+------------------+--------------------+-------------------+\n",
            "|               Email|             Address|          Avatar|Avg Session Length|       Time on App|   Time on Website|Length of Membership|Yearly Amount Spent|\n",
            "+--------------------+--------------------+----------------+------------------+------------------+------------------+--------------------+-------------------+\n",
            "|mstephenson@ferna...|835 Frank TunnelW...|          Violet| 34.49726772511229| 12.65565114916675| 39.57766801952616|  4.0826206329529615|  587.9510539684005|\n",
            "|   hduke@hotmail.com|4547 Archer Commo...|       DarkGreen| 31.92627202636016|11.109460728682564|37.268958868297744|    2.66403418213262|  392.2049334443264|\n",
            "|    pallen@yahoo.com|24645 Valerie Uni...|          Bisque|33.000914755642675|11.330278057777512|37.110597442120856|   4.104543202376424| 487.54750486747207|\n",
            "|riverarebecca@gma...|1414 David Throug...|     SaddleBrown| 34.30555662975554|13.717513665142507| 36.72128267790313|   3.120178782748092|  581.8523440352177|\n",
            "|mstephens@davidso...|14023 Rodriguez P...|MediumAquaMarine| 33.33067252364639|12.795188551078114| 37.53665330059473|   4.446308318351434|  599.4060920457634|\n",
            "|alvareznancy@luca...|645 Martha Park A...|     FloralWhite|33.871037879341976|12.026925339755056| 34.47687762925054|   5.493507201364199|   637.102447915074|\n",
            "|katherine20@yahoo...|68388 Reyes Light...|   DarkSlateBlue| 32.02159550138701|11.366348309710526| 36.68377615286961|   4.685017246570912|  521.5721747578274|\n",
            "|  awatkins@yahoo.com|Unit 6538 Box 898...|            Aqua|32.739142938380326| 12.35195897300293| 37.37335885854755|  4.4342734348999375|  549.9041461052942|\n",
            "|vchurch@walter-ma...|860 Lee KeyWest D...|          Salmon| 33.98777289568564|13.386235275676436|37.534497341555735|  3.2734335777477144|  570.2004089636196|\n",
            "|    bonnie69@lin.biz|PSC 2734, Box 525...|           Brown|31.936548618448917|11.814128294972196| 37.14516822352819|   3.202806071553459|  427.1993848953282|\n",
            "|andrew06@peterson...|26104 Alexander G...|          Tomato|33.992572774953736|13.338975447662113| 37.22580613162114|   2.482607770510596|  492.6060127179966|\n",
            "|ryanwerner@freema...|Unit 2413 Box 034...|          Tomato| 33.87936082480498|11.584782999535266| 37.08792607098381|    3.71320920294043|  522.3374046069357|\n",
            "|   knelson@gmail.com|6705 Miller Orcha...|       RoyalBlue|29.532428967057943|10.961298400154098| 37.42021557502538|   4.046423164299585|  408.6403510726275|\n",
            "|wrightpeter@yahoo...|05302 Dunlap Ferr...|          Bisque| 33.19033404372265|12.959226091609382|36.144666700041924|   3.918541839158999|  573.4158673313865|\n",
            "|taylormason@gmail...|7773 Powell Sprin...|        DarkBlue|32.387975853153876|13.148725692056516| 36.61995708279922|   2.494543646659249|  470.4527333009554|\n",
            "| jstark@anderson.com|49558 Ramirez Roa...|            Peru|30.737720372628182|12.636606052000127|36.213763093698624|  3.3578468423262944|  461.7807421962299|\n",
            "| wjennings@gmail.com|6362 Wilson Mount...|      PowderBlue| 32.12538689728784|11.733861690857394|  34.8940927514398|  3.1361327164897803| 457.84769594494855|\n",
            "|rebecca45@hale-ba...|8982 Burton RowWi...|       OliveDrab|32.338899323067196|12.013194694014402| 38.38513659413844|   2.420806160901484| 407.70454754954415|\n",
            "|alejandro75@hotma...|64475 Andre Club ...|            Cyan|32.187812045932155|  14.7153875441565| 38.24411459434352|   1.516575580831944|  452.3156754800354|\n",
            "|samuel46@love-wes...|544 Alexander Hei...|   LightSeaGreen| 32.61785606282345|13.989592555825254|37.190503800397956|   4.064548550437977|   605.061038804892|\n",
            "+--------------------+--------------------+----------------+------------------+------------------+------------------+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wvj5mAc_3Zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43134c10-4ed2-46d7-8239-21ecd461e799"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Email='mstephenson@fernandez.com', Address='835 Frank TunnelWrightmouth, MI 82180-9605', Avatar='Violet', Avg Session Length=34.49726772511229, Time on App=12.65565114916675, Time on Website=39.57766801952616, Length of Membership=4.0826206329529615, Yearly Amount Spent=587.9510539684005)"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfYqP_s7_6k5"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm2cwwRcADyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bbf48d9-33ec-47cd-8d47-3b7e006dca16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Email',\n",
              " 'Address',\n",
              " 'Avatar',\n",
              " 'Avg Session Length',\n",
              " 'Time on App',\n",
              " 'Time on Website',\n",
              " 'Length of Membership',\n",
              " 'Yearly Amount Spent']"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdDcgXSgAEfj"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=['Avg Session Length', 'Time on App',\n",
        "                                       'Time on Website','Length of Membership'],\n",
        "                            outputCol='features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvX56r21ATEo"
      },
      "outputs": [],
      "source": [
        "output = assembler.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWwaB490AVFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3abf52c-a126-44e6-8fcc-2ab37191e565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|[34.4972677251122...|\n",
            "|[31.9262720263601...|\n",
            "|[33.0009147556426...|\n",
            "|[34.3055566297555...|\n",
            "|[33.3306725236463...|\n",
            "|[33.8710378793419...|\n",
            "|[32.0215955013870...|\n",
            "|[32.7391429383803...|\n",
            "|[33.9877728956856...|\n",
            "|[31.9365486184489...|\n",
            "|[33.9925727749537...|\n",
            "|[33.8793608248049...|\n",
            "|[29.5324289670579...|\n",
            "|[33.1903340437226...|\n",
            "|[32.3879758531538...|\n",
            "|[30.7377203726281...|\n",
            "|[32.1253868972878...|\n",
            "|[32.3388993230671...|\n",
            "|[32.1878120459321...|\n",
            "|[32.6178560628234...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "output.select(\"features\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hY8E1x0FAYCD"
      },
      "outputs": [],
      "source": [
        "final_data = output.select(\"features\", \"Yearly Amount Spent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VUDf8omAfU-"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = final_data.randomSplit([0.7, 0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSuhK_i5AlE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f5c005-0436-41a2-b690-923300d481e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+\n",
            "|summary|Yearly Amount Spent|\n",
            "+-------+-------------------+\n",
            "|  count|                364|\n",
            "|   mean|  502.9869765747741|\n",
            "| stddev|  78.20081373178272|\n",
            "|    min| 298.76200786180766|\n",
            "|    max|  765.5184619388373|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_data.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQbUF7s2Am7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d272a06-ab64-4bc4-add0-e4d7489d1fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+\n",
            "|summary|Yearly Amount Spent|\n",
            "+-------+-------------------+\n",
            "|  count|                136|\n",
            "|   mean|  489.4835268829236|\n",
            "| stddev|  81.70383291336739|\n",
            "|    min| 256.67058229005585|\n",
            "|    max|  669.9871405017029|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_data.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nao_FJx3ArMB"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONZq04cvA5So"
      },
      "outputs": [],
      "source": [
        "lr = LinearRegression(labelCol='Yearly Amount Spent')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRYrz9zKA8yc"
      },
      "outputs": [],
      "source": [
        "model = lr.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKawWONDA_gP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1et5UqYlBGvO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "684a390e-831c-4cae-8275-55e7bb390529"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      Coefficients\n",
              "Avg Session Length       25.528189\n",
              "Time on App              38.739248\n",
              "Time on Website           0.381506\n",
              "Length of Membership     61.116148"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ffc753b2-dc61-4488-b1f2-6cb99c3e40fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Coefficients</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Avg Session Length</th>\n",
              "      <td>25.528189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time on App</th>\n",
              "      <td>38.739248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time on Website</th>\n",
              "      <td>0.381506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Length of Membership</th>\n",
              "      <td>61.116148</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffc753b2-dc61-4488-b1f2-6cb99c3e40fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ffc753b2-dc61-4488-b1f2-6cb99c3e40fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ffc753b2-dc61-4488-b1f2-6cb99c3e40fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-06fb1b00-7d03-4530-953e-df7e46131ad7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-06fb1b00-7d03-4530-953e-df7e46131ad7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-06fb1b00-7d03-4530-953e-df7e46131ad7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"                                       'Time on Website','Length of Membership'])\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Coefficients\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25.387216901316908,\n        \"min\": 0.38150600665850126,\n        \"max\": 61.11614831552337,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          38.73924794022958,\n          61.11614831552337,\n          25.528189448857677\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 305
        }
      ],
      "source": [
        "pd.DataFrame({\"Coefficients\":model.coefficients}, index=['Avg Session Length', 'Time on App',\n",
        "                                       'Time on Website','Length of Membership'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IewgJbnvBNnC"
      },
      "outputs": [],
      "source": [
        "res = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUX-4czHBdrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68559144-ad20-48ba-8dac-079eced6ab9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|           residuals|\n",
            "+--------------------+\n",
            "|   9.669149274977144|\n",
            "| -13.253743124870539|\n",
            "|  -7.014907673268283|\n",
            "|  -18.93401211450589|\n",
            "|   3.666788132691522|\n",
            "| -13.506343132392146|\n",
            "| -7.6506770546077405|\n",
            "|   8.515390472010267|\n",
            "|  18.287686613130802|\n",
            "|  3.2878745843414663|\n",
            "|-0.09345137585660268|\n",
            "| -5.8919286856935855|\n",
            "|  -6.045290947261833|\n",
            "| -17.994114323187887|\n",
            "| -14.543665761366128|\n",
            "| -2.7649679910356895|\n",
            "|   6.936300111304206|\n",
            "| -10.178724246025183|\n",
            "|0.017993930797047142|\n",
            "|   7.475084221903728|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "res.residuals.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UELyAw2oBfyP"
      },
      "outputs": [],
      "source": [
        "unlabeled_data = test_data.select(\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i51SRKfXBm_4"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(unlabeled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39HSr01XBqKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49cca61-75e1-48bf-e327-3389e81ec8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+\n",
            "|            features|        prediction|\n",
            "+--------------------+------------------+\n",
            "|[29.5324289670579...|398.97120179765034|\n",
            "|[30.3931845423455...|333.18261292806415|\n",
            "|[30.4925366965402...| 289.4861533931828|\n",
            "|[30.8162006488763...| 285.0203530629749|\n",
            "|[31.0472221394875...| 388.8306110563299|\n",
            "|[31.0662181616375...| 462.4396363400665|\n",
            "|[31.1280900496166...| 564.9033638016624|\n",
            "|[31.1695067987115...|418.84114033028254|\n",
            "|[31.3123495994443...| 445.3037314148098|\n",
            "|[31.3584771924370...|491.88807586513394|\n",
            "|[31.3895854806643...| 410.1630624358395|\n",
            "|[31.5171218025062...| 281.8103493360793|\n",
            "|[31.5257524169682...| 450.0109177571437|\n",
            "|[31.5702008293202...| 563.9396064645928|\n",
            "|[31.5741380228732...|  558.952937921953|\n",
            "|[31.8186165667690...|449.18364136117134|\n",
            "|[31.8209982016720...|417.73898090190914|\n",
            "|[31.8279790554652...| 450.1814717929667|\n",
            "|[31.8293464559211...|385.13434405717794|\n",
            "|[31.8512531286083...|465.51716244489467|\n",
            "+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZUm7bulBrHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9520cd33-349e-4e58-f215-d3bfd335048b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 7.7682724106361\n",
            "MSE: 92.46118689887506\n",
            "RMSE: 9.615674022078487\n",
            "R2 0.9860466096979039\n",
            "Adj R2 0.985620551978756\n"
          ]
        }
      ],
      "source": [
        "print(\"MAE:\", res.meanAbsoluteError)\n",
        "print(\"MSE:\", res.meanSquaredError)\n",
        "print(\"RMSE:\", res.rootMeanSquaredError)\n",
        "print(\"R2\", res.r2)\n",
        "print(\"Adj R2\", res.r2adj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfKkRWgDcrJ_"
      },
      "source": [
        "# Logistic Regression with PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utJK6INtcuYw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crtgTDKSfVtP"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"log_reg\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw5XB_xxgVnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed577a20-c9ee-44b9-ea54-6a07cbbb6af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  102k  100  102k    0     0   467k      0 --:--:-- --:--:-- --:--:--  467k\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt >> sample_libsvm_data_2.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyY3E2z5faXW"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"libsvm\").load(\"sample_libsvm_data_2.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk4AUM4tfk29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee30360-cee0-4bcc-b284-bd516c70cb9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- label: double (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv5APJp_gsDU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU33l08cg1ba"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\n",
        "\n",
        "model = lr.fit(df)\n",
        "\n",
        "summary = model.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSeNblqsg79x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c73d2fb-6d49-437f-c27b-9c6e6cc6f0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|  0.0|(692,[127,128,129...|[20.3777627514862...|[0.99999999858729...|       0.0|\n",
            "|  1.0|(692,[158,159,160...|[-21.114014198867...|[6.76550380001560...|       1.0|\n",
            "|  1.0|(692,[124,125,126...|[-23.743613234676...|[4.87842678715831...|       1.0|\n",
            "|  1.0|(692,[152,153,154...|[-19.192574012719...|[4.62137287298722...|       1.0|\n",
            "|  1.0|(692,[151,152,153...|[-20.125398874697...|[1.81823629113437...|       1.0|\n",
            "|  0.0|(692,[129,130,131...|[20.4890549504187...|[0.99999999873608...|       0.0|\n",
            "|  1.0|(692,[158,159,160...|[-21.082940212813...|[6.97903542824686...|       1.0|\n",
            "|  1.0|(692,[99,100,101,...|[-19.622713503566...|[3.00582577441380...|       1.0|\n",
            "|  0.0|(692,[154,155,156...|[21.1594863606570...|[0.99999999935352...|       0.0|\n",
            "|  0.0|(692,[127,128,129...|[28.1036706837273...|[0.99999999999937...|       0.0|\n",
            "|  1.0|(692,[154,155,156...|[-21.054076780105...|[7.18340962960684...|       1.0|\n",
            "|  0.0|(692,[153,154,155...|[26.9648490510173...|[0.99999999999805...|       0.0|\n",
            "|  0.0|(692,[151,152,153...|[32.7855654161393...|[0.99999999999999...|       0.0|\n",
            "|  1.0|(692,[129,130,131...|[-20.331839179665...|[1.47908944090011...|       1.0|\n",
            "|  0.0|(692,[154,155,156...|[21.7830579106565...|[0.99999999965347...|       0.0|\n",
            "|  1.0|(692,[150,151,152...|[-20.640562103727...|[1.08621994880504...|       1.0|\n",
            "|  0.0|(692,[124,125,126...|[22.6400775503740...|[0.99999999985292...|       0.0|\n",
            "|  0.0|(692,[152,153,154...|[38.0712919910898...|           [1.0,0.0]|       0.0|\n",
            "|  1.0|(692,[97,98,99,12...|[-19.830803265627...|[2.44113371545821...|       1.0|\n",
            "|  1.0|(692,[124,125,126...|[-21.016054806035...|[7.46179590485056...|       1.0|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "summary.predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcMpxsVWhAaN"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgj2p-lEhJ3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f7c737-003d-4fac-ea7a-c0202bc3a5a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.ml.classification.BinaryLogisticRegressionSummary at 0x7d884b11d540>"
            ]
          },
          "metadata": {},
          "execution_count": 322
        }
      ],
      "source": [
        "model.evaluate(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S385rAdshL1O"
      },
      "outputs": [],
      "source": [
        "pred_and_labels = model.evaluate(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7vsBpV7hTUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5200ce-0a27-4a61-faf0-e1f7cf319a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|  0.0|(692,[127,128,129...|[20.3777627514862...|[0.99999999858729...|       0.0|\n",
            "|  1.0|(692,[158,159,160...|[-21.114014198867...|[6.76550380001560...|       1.0|\n",
            "|  1.0|(692,[124,125,126...|[-23.743613234676...|[4.87842678715831...|       1.0|\n",
            "|  1.0|(692,[152,153,154...|[-19.192574012719...|[4.62137287298722...|       1.0|\n",
            "|  1.0|(692,[151,152,153...|[-20.125398874697...|[1.81823629113437...|       1.0|\n",
            "|  0.0|(692,[129,130,131...|[20.4890549504187...|[0.99999999873608...|       0.0|\n",
            "|  1.0|(692,[158,159,160...|[-21.082940212813...|[6.97903542824686...|       1.0|\n",
            "|  1.0|(692,[99,100,101,...|[-19.622713503566...|[3.00582577441380...|       1.0|\n",
            "|  0.0|(692,[154,155,156...|[21.1594863606570...|[0.99999999935352...|       0.0|\n",
            "|  0.0|(692,[127,128,129...|[28.1036706837273...|[0.99999999999937...|       0.0|\n",
            "|  1.0|(692,[154,155,156...|[-21.054076780105...|[7.18340962960684...|       1.0|\n",
            "|  0.0|(692,[153,154,155...|[26.9648490510173...|[0.99999999999805...|       0.0|\n",
            "|  0.0|(692,[151,152,153...|[32.7855654161393...|[0.99999999999999...|       0.0|\n",
            "|  1.0|(692,[129,130,131...|[-20.331839179665...|[1.47908944090011...|       1.0|\n",
            "|  0.0|(692,[154,155,156...|[21.7830579106565...|[0.99999999965347...|       0.0|\n",
            "|  1.0|(692,[150,151,152...|[-20.640562103727...|[1.08621994880504...|       1.0|\n",
            "|  0.0|(692,[124,125,126...|[22.6400775503740...|[0.99999999985292...|       0.0|\n",
            "|  0.0|(692,[152,153,154...|[38.0712919910898...|           [1.0,0.0]|       0.0|\n",
            "|  1.0|(692,[97,98,99,12...|[-19.830803265627...|[2.44113371545821...|       1.0|\n",
            "|  1.0|(692,[124,125,126...|[-21.016054806035...|[7.46179590485056...|       1.0|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred_and_labels.predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnTmLRlthVq2"
      },
      "outputs": [],
      "source": [
        "pred_and_labels = pred_and_labels.predictions.select(\"label\", \"prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmCPPQ1Lhgu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "575c401a-948e-4d81-9419-d8747d51d17f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  0.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       1.0|\n",
            "|  0.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       1.0|\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       1.0|\n",
            "|  0.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred_and_labels.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AMd1FHPhldi"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkLOsocfhh4X"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjCTMjK9hsUT"
      },
      "outputs": [],
      "source": [
        "eval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ-6RIaDh3Ga"
      },
      "outputs": [],
      "source": [
        "eval_multi = MulticlassClassificationEvaluator(predictionCol=\"prediction\",\n",
        "                                               labelCol=\"label\",\n",
        "                                               metricName=\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZtRtIKrh60w"
      },
      "outputs": [],
      "source": [
        "acc = eval.evaluate(pred_and_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUdz7yu-iN6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ad08d8-2b3d-405b-8620-5e62d1e9003b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 332
        }
      ],
      "source": [
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbSMeqg3i1ff"
      },
      "source": [
        "# Logistic Regression: Titantic Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTGS-LpOiOg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bf75c6-f536-42f6-a509-ca312c6ee7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 60302  100 60302    0     0   378k      0 --:--:-- --:--:-- --:--:--  379k\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/markumreed/colab_pyspark/main/titanic.csv >> titanic.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kvnjap_i706"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTAdSyM4jELE"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"titanic\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUDa2AStjLDM"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"titanic.csv\", inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI-TwAF8jRb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a024e0f2-391b-4f96-daaa-7be2d88fe894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: integer (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzulCYEQjWez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f8fd07-4a6f-43b3-ae35-0da13e668447"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PassengerId',\n",
              " 'Survived',\n",
              " 'Pclass',\n",
              " 'Name',\n",
              " 'Sex',\n",
              " 'Age',\n",
              " 'SibSp',\n",
              " 'Parch',\n",
              " 'Ticket',\n",
              " 'Fare',\n",
              " 'Cabin',\n",
              " 'Embarked']"
            ]
          },
          "metadata": {},
          "execution_count": 344
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yl6cSyDjdnH"
      },
      "outputs": [],
      "source": [
        "data = df.select([\n",
        " 'Survived',\n",
        " 'Pclass',\n",
        " 'Sex',\n",
        " 'Age',\n",
        " 'SibSp',\n",
        " 'Parch',\n",
        " 'Fare',\n",
        " 'Embarked'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XKalFdxkAB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970f2f7b-556c-40e6-9acb-2b3873840bd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Survived=0, Pclass=3, Sex='male', Age=22.0, SibSp=1, Parch=0, Fare=7.25, Embarked='S')"
            ]
          },
          "metadata": {},
          "execution_count": 346
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7CejI1mj59W"
      },
      "outputs": [],
      "source": [
        "data_final = data.na.drop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYDgN12HkFR6"
      },
      "source": [
        "# Categorical Data with PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J13ihqpBj-4f"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import (VectorAssembler, VectorIndexer,\n",
        "                                OneHotEncoder, StringIndexer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHiuHDtKkOIZ"
      },
      "outputs": [],
      "source": [
        "gender_indexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
        "gender_ecoder = OneHotEncoder(inputCol=\"SexIndex\", outputCol=\"SexVec\")\n",
        "\n",
        "embark_indexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkIndex\")\n",
        "embark_ecoder = OneHotEncoder(inputCol=\"EmbarkIndex\", outputCol=\"EmbarkVec\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlRyypOtkpAP"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=[\"Pclass\", \"SexVec\", \"Age\", \"SibSp\",\n",
        "                                       \"Parch\", \"Fare\", \"EmbarkVec\"],\n",
        "                            outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLiFP72Hk54r"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD-lhON6lME5"
      },
      "source": [
        "# Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TCiOKO1lLd2"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsE3nMEmlQ9Z"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(featuresCol='features', labelCol=\"Survived\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n21fJ24jlfix"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages=[\n",
        "                            gender_indexer,embark_indexer,\n",
        "                            gender_ecoder,embark_ecoder,\n",
        "                            assembler, lr\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxahC2GRl3Tj"
      },
      "outputs": [],
      "source": [
        "train, test = data_final.randomSplit([0.7, 0.3], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x3xjhTOmBJr"
      },
      "outputs": [],
      "source": [
        "model_fit = pipeline.fit(train)\n",
        "res = model_fit.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUAIFA-KmJ9Y"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgxCK1BYmQ17"
      },
      "outputs": [],
      "source": [
        "eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
        "                                     labelCol='Survived')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT3hPq-5mW_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce4ae4e-8143-4d81-9965-ca123270814d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+\n",
            "|Survived|prediction|\n",
            "+--------+----------+\n",
            "|       0|       1.0|\n",
            "|       0|       1.0|\n",
            "|       0|       1.0|\n",
            "|       0|       1.0|\n",
            "|       0|       1.0|\n",
            "|       0|       0.0|\n",
            "|       0|       1.0|\n",
            "|       0|       1.0|\n",
            "|       0|       1.0|\n",
            "|       0|       1.0|\n",
            "|       0|       1.0|\n",
            "|       0|       0.0|\n",
            "|       0|       0.0|\n",
            "|       0|       0.0|\n",
            "|       0|       0.0|\n",
            "|       0|       1.0|\n",
            "|       0|       0.0|\n",
            "|       0|       0.0|\n",
            "|       0|       0.0|\n",
            "|       0|       1.0|\n",
            "+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "res.select('Survived', 'prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yp-93mSmejC"
      },
      "outputs": [],
      "source": [
        "auc = eval.evaluate(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aNRTQJimn5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714960da-8cf6-4643-f475-75863d260c5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7747561675272518"
            ]
          },
          "metadata": {},
          "execution_count": 363
        }
      ],
      "source": [
        "auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa2UXWygtuUY"
      },
      "source": [
        "# Clustering with PySpark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncv8wIkTtxAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704aa6df-ba00-41f7-ae78-1ae2ab78d06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   120  100   120    0     0    863      0 --:--:-- --:--:-- --:--:--   869\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_kmeans_data.txt >> sample_kmeans_data.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESyjs3N4yR6V"
      },
      "source": [
        "# K-means\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmcPjc96uT7s"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"sample_cluster\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnSgan58ygYe"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDFRgivtyjH6"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"libsvm\").load(\"sample_kmeans_data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzONl_rYyqdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840d88ea-8884-4cfb-b540-90a0c80d6506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|           (3,[],[])|\n",
            "|  1.0|(3,[0,1,2],[0.1,0...|\n",
            "|  2.0|(3,[0,1,2],[0.2,0...|\n",
            "|  3.0|(3,[0,1,2],[9.0,9...|\n",
            "|  4.0|(3,[0,1,2],[9.1,9...|\n",
            "|  5.0|(3,[0,1,2],[9.2,9...|\n",
            "+-----+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yPmptZmytFd"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans().setK(2).setSeed(42)\n",
        "model = kmeans.fit(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfgQJvcvy5-M"
      },
      "outputs": [],
      "source": [
        "pred = model.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-jAdjhazCOE"
      },
      "outputs": [],
      "source": [
        "eval = ClusteringEvaluator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAKHieBn0Ypa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07d0c47-1d7d-4111-d607-efdffbce9d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance: 0.9997530305375207\n"
          ]
        }
      ],
      "source": [
        "silhouette = eval.evaluate(pred)\n",
        "print(f\"Silhouette with squared euclidean distance: {silhouette}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzo7NkN80m3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322ef95a-c585-4275-a193-8a38eb024369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers:\n",
            "=================\n",
            "[0.1 0.1 0.1]\n",
            "[9.1 9.1 9.1]\n"
          ]
        }
      ],
      "source": [
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers:\")\n",
        "print(\"=================\")\n",
        "for center in centers:\n",
        "  print(center)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMXHNbOTuIZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3e7103-bbbd-4020-866c-3df8d23bb603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  9300    0  9300    0     0  28174      0 --:--:-- --:--:-- --:--:-- 28267\n"
          ]
        }
      ],
      "source": [
        "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt >> seeds_dataset.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7bRUbGkj5Mz"
      },
      "source": [
        "# Random Forest Classifier with PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keb9diwMj88b"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beY9VaBVkGFb"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"rf\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCYpsAvZkLcB"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yek_YHJ1kTpP"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCai6Xl4kry3"
      },
      "source": [
        "# Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGOooXf-kVZX"
      },
      "outputs": [],
      "source": [
        "(train, test) = df.randomSplit([0.7, 0.3], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaJ79ozpkiJs"
      },
      "outputs": [],
      "source": [
        "test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzmudBvYkjE7"
      },
      "outputs": [],
      "source": [
        "train.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfyGOiTsktt3"
      },
      "source": [
        "# Train RF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86bnZm5Qkn0J"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20,seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iBFroidk74m"
      },
      "outputs": [],
      "source": [
        "model = rf.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFUU5DBYk-C6"
      },
      "outputs": [],
      "source": [
        "pred = model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akcaewHalAq-"
      },
      "outputs": [],
      "source": [
        "pred.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXxxrfr0lCkP"
      },
      "outputs": [],
      "source": [
        "pred.select(\"prediction\", \"label\", \"features\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zvcyWgslNqB"
      },
      "outputs": [],
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKIZLEo4lXtv"
      },
      "outputs": [],
      "source": [
        "acc = eval.evaluate(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WngyCZs3la9V"
      },
      "outputs": [],
      "source": [
        "print(\"Test Error = %g\" % (1.0 - acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kHlr0bJlmru"
      },
      "outputs": [],
      "source": [
        "model.featureImportances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVis_fO8mCZp"
      },
      "source": [
        "# Gradient Boosted Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2nGI6y-lwq3"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import GBTClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6V4UUIXmghD"
      },
      "outputs": [],
      "source": [
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8R36fQAmiWJ"
      },
      "outputs": [],
      "source": [
        "model = gbt.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTMHSAonm5Oj"
      },
      "outputs": [],
      "source": [
        "pred = model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzPYr8r2m7sx"
      },
      "outputs": [],
      "source": [
        "pred.select(\"prediction\", \"label\", \"features\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0muS4ERxnAo-"
      },
      "outputs": [],
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "acc = eval.evaluate(pred)\n",
        "print(\"Test Error = %g\" % (1.0 - acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuBAmQHL2vXh"
      },
      "source": [
        "## Tree Methods with PySpark\n",
        "1. Single Decision Tree\n",
        "1. Random Forest\n",
        "1. Gradient Boosted Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxHdtYJ7nOf7"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"trees\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_Uu4nY44pLu"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"College.csv\", inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7EKhh7O4teO"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCf502xw5GK3"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj2e0-oP4vg1"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMm8Udal5EXo"
      },
      "source": [
        "# Formatting for Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-fQbbcB5CTI"
      },
      "outputs": [],
      "source": [
        "# \"label\", \"features\"\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l9yFfNk5jUc"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL7wctqo5V63"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcceDpLR5aO0"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=['Apps',\n",
        " 'Accept',\n",
        " 'Enroll',\n",
        " 'Top10perc',\n",
        " 'Top25perc',\n",
        " 'F_Undergrad',\n",
        " 'P_Undergrad',\n",
        " 'Outstate',\n",
        " 'Room_Board',\n",
        " 'Books',\n",
        " 'Personal',\n",
        " 'PhD',\n",
        " 'Terminal',\n",
        " 'S_F_Ratio',\n",
        " 'perc_alumni',\n",
        " 'Expend',\n",
        " 'Grad_Rate'\n",
        "    ],\n",
        "    outputCol=\"features\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i96o3Zcp5zdG"
      },
      "outputs": [],
      "source": [
        "output = assembler.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6E-fj_R6H7s"
      },
      "source": [
        "# String Variables (Private)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPxGjsm86G5h"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSKIWKJp6M6D"
      },
      "outputs": [],
      "source": [
        "indexer = StringIndexer(inputCol=\"Private\", outputCol=\"PrivateIndexer\")\n",
        "output_fixed = indexer.fit(output).transform(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFwIN9KS6aoe"
      },
      "outputs": [],
      "source": [
        "df_final = output_fixed.select(\"features\", \"PrivateIndexer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCptatjo60bf"
      },
      "outputs": [],
      "source": [
        "train, test = df_final.randomSplit([0.7, 0.3], seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5vHh5OK66ZM"
      },
      "source": [
        "# Tree Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nao7hys65wz"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy-tu65V7JUs"
      },
      "source": [
        "## CREATE Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATFKKoMr7IZV"
      },
      "outputs": [],
      "source": [
        "dtc = DecisionTreeClassifier(labelCol=\"PrivateIndexer\", featuresCol=\"features\")\n",
        "rfc = RandomForestClassifier(labelCol=\"PrivateIndexer\", featuresCol=\"features\")\n",
        "gbt = GBTClassifier(labelCol=\"PrivateIndexer\", featuresCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl3S4CQz7Y6m"
      },
      "outputs": [],
      "source": [
        "dtc_model = dtc.fit(train)\n",
        "rfc_model = rfc.fit(train)\n",
        "gbt_model = gbt.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV8MSude71g1"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNfI3iuB7iYK"
      },
      "outputs": [],
      "source": [
        "dtc_pred = dtc_model.transform(test)\n",
        "rfc_pred = rfc_model.transform(test)\n",
        "gbt_pred = gbt_model.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P17RRuoO70CM"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGr2KfOb7y_l"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8IhNGMn76N6"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"PrivateIndexer\", predictionCol=\"prediction\", metricName=\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRj3g7y-8G4M"
      },
      "outputs": [],
      "source": [
        "dtc_acc = evaluator.evaluate(dtc_pred)\n",
        "rfc_acc = evaluator.evaluate(rfc_pred)\n",
        "gbt_acc = evaluator.evaluate(gbt_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qVkFO758W3M"
      },
      "outputs": [],
      "source": [
        "print(\"-\"*10)\n",
        "print(f\"DT Acc: {dtc_acc}\")\n",
        "print(\"-\"*10)\n",
        "print(f\"RFC Acc: {rfc_acc}\")\n",
        "print(\"-\"*10)\n",
        "print(f\"GBT Acc: {gbt_acc}\")\n",
        "print(\"-\"*10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NTXmbJX8byX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7ULQkBngS6gQ",
        "3AMd1FHPhldi",
        "RbSMeqg3i1ff",
        "wYDgN12HkFR6",
        "Sa2UXWygtuUY",
        "x7bRUbGkj5Mz",
        "WCai6Xl4kry3",
        "dfyGOiTsktt3",
        "pVis_fO8mCZp",
        "XMm8Udal5EXo",
        "w6E-fj_R6H7s",
        "B5vHh5OK66ZM",
        "IV8MSude71g1",
        "P17RRuoO70CM"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}