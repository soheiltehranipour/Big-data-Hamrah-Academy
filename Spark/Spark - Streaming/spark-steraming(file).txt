docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Ports}} "
# ------------------------------
docker exec -it spark-master /bin/bash
# ------------------------------
/spark/bin/pyspark
# ------------------------------
from pyspark.streaming import StreamingContext
ssc = StreamingContext(sc, 2)

lines = ssc.textFileStream(r'/tmp/test_stream')

# Split each line into words
words = lines.flatMap(lambda line: line.split(" "))

# Count each word in each batch
pairs = words.map(lambda word: (word, 1))
wordCounts = pairs.reduceByKey(lambda x, y: x + y)

# Print the first ten elements of each RDD generated in this DStream to the console
wordCounts.pprint()

# Start the computation
ssc.start()

# Wait for the computation to terminate
# ssc.awaitTermination()

# ------------------------------From namenode ---------
docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Ports}} "
# ------------------------------
docker exec -it namenode /bin/bash
hdfs dfs -mkdir /test_stream
hdfs dfs -ls /
# ------------------------------
/spark/bin/pyspark --master spark://spark-master:7077
# ------------------------------
from pyspark.streaming import StreamingContext
ssc = StreamingContext(sc, 2)
lines = ssc.textFileStream(r'hdfs://namenode:9000/test_stream')
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
wordCounts = pairs.reduceByKey(lambda x, y: x + y)
wordCounts.pprint()
ssc.start()

echo '1 2 3' > test1.txt
hdfs dfs -put test1.txt /test_stream